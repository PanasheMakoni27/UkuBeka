<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Flask App</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <div class="container">
        <button class="theme-toggle" id="themeToggle" title="Toggle light/dark mode">Light/Dark</button>
        <div class="header">
            <span class="header-icon"></span>
            <span>Live Recognition</span>
        </div>
        <div class="webcam-area glow-red" id="webcamArea" style="position:relative;">
            <video id="webcam" autoplay playsinline muted style="position:absolute;top:0;left:0;width:100%;height:100%;z-index:1;background:#000;"></video>
            <canvas id="overlay" style="position:absolute;top:0;left:0;width:100%;height:100%;z-index:2;"></canvas>
        </div>
        <div class="results-area" id="results">
            <div class="spinner" id="spinner"></div>
            <div class="pulse-text" id="waitingText">Waiting for gesture...</div>
        </div>
        <div class="translation-panel" id="translationPanel">
            <span class="translation-label">Translation:</span>
            <span class="translation-word" id="translationWord">---</span>
        </div>
        <div class="translation-panel" id="translatedTextPanel">
            <span class="translation-label">Gesture to Text:</span>
            <span class="translation-word" id="translatedText">Waiting for gestureâ€¦</span>
        </div>
    </div>
    <!-- MediaPipe Hands and Drawing Utils -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <script>
        const video = document.getElementById('webcam');
        const canvas = document.getElementById('overlay');
        const ctx = canvas.getContext('2d');
        const webcamArea = document.getElementById('webcamArea');
        const resultsArea = document.getElementById('results');
        const spinner = document.getElementById('spinner');
        const waitingText = document.getElementById('waitingText');
        const translationWord = document.getElementById('translationWord');
        const translatedText = document.getElementById('translatedText');
        let handDetected = false;
        let lastGesture = '';
        // Theme toggle
        const themeToggle = document.getElementById('themeToggle');
        themeToggle.onclick = () => {
            const isDark = document.body.getAttribute('data-theme') === 'dark';
            document.body.setAttribute('data-theme', isDark ? 'light' : 'dark');
            themeToggle.textContent = isDark ? 'ðŸŒ™' : 'â˜€ï¸';
        };
        // Set default theme
        if (!document.body.getAttribute('data-theme')) {
            document.body.setAttribute('data-theme', 'light');
        }
        // Resize canvas to match video
        function resizeCanvas() {
            if (video.videoWidth > 0 && video.videoHeight > 0) {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
            }
        }
        // Access webcam
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
            navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' }, audio: false })
                .then(function(stream) {
                    video.srcObject = stream;
                    video.onloadedmetadata = () => {
                        video.play();
                        resizeCanvas();
                    };
                })
                .catch(function(err) {
                    console.error('Webcam error:', err);
                    resultsArea.innerHTML = '<span style="color:red">Webcam access denied or not available.<br>' + err.message + '</span>';
                });
        } else {
            resultsArea.innerHTML = '<span style="color:red">Webcam not supported by this browser.</span>';
        }
        // Example gesture classifier (replace with your model or backend call)
        function classifyGesture(flatLandmarks) {
            // Simple ruleset for demonstration (replace with ML model or backend call)
            // Example: If the hand is open (distance between wrist and middle finger tip is large), return 'Hello'
            // If the hand is closed (distance is small), return 'Fist'
            // This is just a placeholder logic
            if (!flatLandmarks || flatLandmarks.length !== 42) return null;
            const wristX = flatLandmarks[0], wristY = flatLandmarks[1];
            const midTipX = flatLandmarks[16], midTipY = flatLandmarks[17]; // 8th landmark (index 8*2)
            const dx = midTipX - wristX;
            const dy = midTipY - wristY;
            const dist = Math.sqrt(dx*dx + dy*dy);
            if (dist > 0.2) return 'Hello';
            if (dist < 0.1) return 'Fist';
            return 'Unknown';
        }

        function preprocessLandmarks(landmarks) {
            // Normalize landmarks to wrist (landmark 0)
            if (!landmarks || landmarks.length !== 21) return null;
            const baseX = landmarks[0].x, baseY = landmarks[0].y;
            let flat = [];
            for (let i = 0; i < 21; i++) {
                flat.push(landmarks[i].x - baseX);
                flat.push(landmarks[i].y - baseY);
            }
            return flat;
        }

        function onResults(results) {
            ctx.save();
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            let landmarkData = [];
            handDetected = false;
            let gestureText = 'Waiting for gestureâ€¦';
            if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
                handDetected = true;
                for (const landmarks of results.multiHandLandmarks) {
                    window.drawConnectors(ctx, landmarks, window.HAND_CONNECTIONS, {color: '#00FF00', lineWidth: 2});
                    window.drawLandmarks(ctx, landmarks, {color: '#FF0000', lineWidth: 1});
                    landmarkData.push(landmarks.map(lm => ({x: lm.x, y: lm.y, z: lm.z})));
                    // Gesture-to-text translation (client-side demo)
                    const flat = preprocessLandmarks(landmarks);
                    const label = classifyGesture(flat);
                    if (label && label !== 'Unknown') gestureText = label;
                }
            }
            ctx.restore();
            // Glowing border
            webcamArea.classList.toggle('glow-green', handDetected);
            webcamArea.classList.toggle('glow-red', !handDetected);
            // Show spinner/pulse if no gesture
            if (!handDetected) {
                spinner.style.display = '';
                waitingText.style.display = '';
                resultsArea.querySelectorAll('.recognized-gesture').forEach(e => e.remove());
                lastGesture = '';
                translationWord.textContent = '---';
                translatedText.textContent = 'Waiting for gestureâ€¦';
                return;
            }
            // Send to backend if any hand detected
            if (landmarkData.length > 0) {
                fetch('/predict', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({hands: landmarkData})
                })
                .then(res => res.json())
                .then(data => {
                    spinner.style.display = 'none';
                    waitingText.style.display = 'none';
                    if (data.prediction && data.prediction !== lastGesture) {
                        lastGesture = data.prediction;
                        // Remove old gesture
                        resultsArea.querySelectorAll('.recognized-gesture').forEach(e => e.remove());
                        // Slide-in animated feedback
                        const gestureDiv = document.createElement('div');
                        gestureDiv.className = 'recognized-gesture';
                        gestureDiv.textContent = `Gesture recognized: ${data.prediction}`;
                        resultsArea.appendChild(gestureDiv);
                        translationWord.textContent = data.prediction;
                        translatedText.textContent = data.prediction;
                        // Voice output
                        if ('speechSynthesis' in window) {
                            const utter = new SpeechSynthesisUtterance(data.prediction);
                            utter.lang = 'en-ZA';
                            window.speechSynthesis.cancel();
                            window.speechSynthesis.speak(utter);
                        }
                    }
                })
                .catch(() => {
                    spinner.style.display = 'none';
                    waitingText.style.display = 'none';
                    resultsArea.innerHTML = '<span style="color:red">Error sending data.</span>';
                    translationWord.textContent = '---';
                    translatedText.textContent = 'Waiting for gestureâ€¦';
                });
            }
        }
        let hands;
        function setupHands() {
            hands = new window.Hands({
                locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
            });
            hands.setOptions({
                maxNumHands: 2,
                modelComplexity: 1,
                minDetectionConfidence: 0.7,
                minTrackingConfidence: 0.7
            });
            hands.onResults(onResults);
        }
        // Remove MediaPipe Camera usage and use requestAnimationFrame loop
        function startHandTracking() {
            if (!hands) return;
            async function processFrame() {
                if (video.readyState === 4 && video.videoWidth > 0 && video.videoHeight > 0) {
                    resizeCanvas();
                    await hands.send({image: video});
                }
                requestAnimationFrame(processFrame);
            }
            processFrame();
        }
        window.addEventListener('DOMContentLoaded', () => {
            setupHands();
            video.addEventListener('loadeddata', startHandTracking);
        });
    </script>
</body>
</html>
