<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Flask App</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <header class="ukubeka-header">
        <div class="ukubeka-logo">
            <!-- Logo removed as requested -->
        </div>
        <div class="ukubeka-title">Gesture-to-Text Communication</div>
    </header>
    <div class="container">
        <button class="theme-toggle" id="themeToggle" title="Toggle light/dark mode"></button>
        <main class="main-flex-panels">
            <section class="gesture-panel gesture-panel-left">
                <h2>Gesture Detection</h2>
                <div class="camera-icon" aria-hidden="true">
                    <svg width="48" height="48" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <rect x="6" y="14" width="36" height="24" rx="4" fill="#4fc3f7"/>
                        <rect x="14" y="8" width="20" height="8" rx="2" fill="#b3e5fc"/>
                        <circle cx="24" cy="26" r="7" fill="#181c24" stroke="#90caf9" stroke-width="2"/>
                        <circle cx="24" cy="26" r="3" fill="#90caf9"/>
                    </svg>
                </div>
                <div class="camera-preview-label">Camera Preview</div>
                <div class="camera-instruction">Start recording to begin gesture detection</div>
                <div class="webcam-area" id="webcamArea">
                    <video id="webcam" autoplay playsinline muted style="position:absolute;top:0;left:0;width:100%;height:100%;z-index:1;background:#000;border-radius:5px;"></video>
                    <canvas id="overlay" style="position:absolute;top:0;left:0;width:100%;height:100%;z-index:2;"></canvas>
                    <div class="webcam-placeholder" id="webcamPlaceholder">Camera unavailable</div>
                </div>
                <button class="start-detection-btn" id="startDetectionBtn">Start Detection</button>
            </section>
            <section class="translated-panel">
                <h2>Translated Text</h2>
                <div class="hand-icon" aria-hidden="true">
                    <svg width="48" height="48" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M14 28V14a4 4 0 1 1 8 0v14" stroke="#81c784" stroke-width="3" stroke-linecap="round"/>
                        <path d="M22 28V10a4 4 0 1 1 8 0v18" stroke="#81c784" stroke-width="3" stroke-linecap="round"/>
                        <path d="M30 28V18a4 4 0 1 1 8 0v10" stroke="#81c784" stroke-width="3" stroke-linecap="round"/>
                        <rect x="10" y="28" width="28" height="10" rx="5" fill="#c8e6c9"/>
                    </svg>
                </div>
                <div class="translated-placeholder">Start gesture detection to see translated text here</div>
                <div class="output-panel">
                    <div class="output-row">
                        <span class="output-label">Translation:</span>
                        <span class="output-value" id="translationWord">---</span>
                    </div>
                    <div class="output-row">
                        <span class="output-label">Pronunciation:</span>
                        <span class="output-value" id="pronunciationWord">---</span>
                    </div>
                </div>
                <button class="clear-btn" id="clearBtn">Clear</button>
            </section>
        </main>
        <section class="session-stats">
            <h3 class="stats-title">Session Statistics</h3>
            <div class="stats-row">
                <div class="stats-label">Words</div>
                <div class="stats-value" id="statsWords">0</div>
            </div>
            <div class="stats-row">
                <div class="stats-label">Accuracy</div>
                <div class="stats-value" id="statsAccuracy">0%</div>
            </div>
            <div class="stats-row">
                <div class="stats-label">Status</div>
                <div class="stats-value" id="statsStatus">Idle</div>
            </div>
        </section>
    </div>
    <!-- MediaPipe Hands and Drawing Utils -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <script>
        const video = document.getElementById('webcam');
        const canvas = document.getElementById('overlay');
        const ctx = canvas.getContext('2d');
        const webcamArea = document.getElementById('webcamArea');
        const resultsArea = document.getElementById('results');
        const spinner = document.getElementById('spinner');
        const waitingText = document.getElementById('waitingText');
        const translationWord = document.getElementById('translationWord');
        const translatedText = document.getElementById('translatedText');
        let handDetected = false;
        let lastGesture = '';
        // Theme toggle
        const themeToggle = document.getElementById('themeToggle');
        themeToggle.onclick = () => {
            const isDark = document.body.getAttribute('data-theme') === 'dark';
            document.body.setAttribute('data-theme', isDark ? 'light' : 'dark');
            themeToggle.textContent = isDark ? 'ðŸŒ™' : 'â˜€ï¸';
        };
        // Set default theme
        if (!document.body.getAttribute('data-theme')) {
            document.body.setAttribute('data-theme', 'light');
        }
        // Resize canvas to match video
        function resizeCanvas() {
            if (video.videoWidth > 0 && video.videoHeight > 0) {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
            }
        }
        // Access webcam
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
            navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' }, audio: false })
                .then(function(stream) {
                    video.srcObject = stream;
                    video.onloadedmetadata = () => {
                        video.play();
                        resizeCanvas();
                    };
                })
                .catch(function(err) {
                    console.error('Webcam error:', err);
                    resultsArea.innerHTML = '<span style="color:red">Webcam access denied or not available.<br>' + err.message + '</span>';
                });
        } else {
            resultsArea.innerHTML = '<span style="color:red">Webcam not supported by this browser.</span>';
        }
        // Example gesture classifier (replace with your model or backend call)
        function classifyGesture(flatLandmarks) {
            // Simple ruleset for demonstration (replace with ML model or backend call)
            // Example: If the hand is open (distance between wrist and middle finger tip is large), return 'Hello'
            // If the hand is closed (distance is small), return 'Fist'
            // This is just a placeholder logic
            if (!flatLandmarks || flatLandmarks.length !== 42) return null;
            const wristX = flatLandmarks[0], wristY = flatLandmarks[1];
            const midTipX = flatLandmarks[16], midTipY = flatLandmarks[17]; // 8th landmark (index 8*2)
            const dx = midTipX - wristX;
            const dy = midTipY - wristY;
            const dist = Math.sqrt(dx*dx + dy*dy);
            if (dist > 0.2) return 'Hello';
            if (dist < 0.1) return 'Fist';
            return 'Unknown';
        }

        function preprocessLandmarks(landmarks) {
            // Normalize landmarks to wrist (landmark 0)
            if (!landmarks || landmarks.length !== 21) return null;
            const baseX = landmarks[0].x, baseY = landmarks[0].y;
            let flat = [];
            for (let i = 0; i < 21; i++) {
                flat.push(landmarks[i].x - baseX);
                flat.push(landmarks[i].y - baseY);
            }
            return flat;
        }

        function onResults(results) {
            ctx.save();
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            let landmarkData = [];
            handDetected = false;
            let gestureText = 'Waiting for gestureâ€¦';
            if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
                handDetected = true;
                for (const landmarks of results.multiHandLandmarks) {
                    window.drawConnectors(ctx, landmarks, window.HAND_CONNECTIONS, {color: '#00FF00', lineWidth: 2});
                    window.drawLandmarks(ctx, landmarks, {color: '#FF0000', lineWidth: 1});
                    landmarkData.push(landmarks.map(lm => ({x: lm.x, y: lm.y, z: lm.z})));
                    // Gesture-to-text translation (client-side demo)
                    const flat = preprocessLandmarks(landmarks);
                    const label = classifyGesture(flat);
                    if (label && label !== 'Unknown') gestureText = label;
                }
            }
            ctx.restore();
            // Glowing border
            webcamArea.classList.toggle('glow-green', handDetected);
            webcamArea.classList.toggle('glow-red', !handDetected);
            // Show spinner/pulse if no gesture
            if (!handDetected) {
                spinner.style.display = '';
                waitingText.style.display = '';
                resultsArea.querySelectorAll('.recognized-gesture').forEach(e => e.remove());
                lastGesture = '';
                translationWord.textContent = '---';
                translatedText.textContent = 'Waiting for gestureâ€¦';
                return;
            }
            // Send to backend if any hand detected
            if (landmarkData.length > 0) {
                fetch('/predict', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({hands: landmarkData})
                })
                .then(res => res.json())
                .then(data => {
                    spinner.style.display = 'none';
                    waitingText.style.display = 'none';
                    if (data.prediction && data.prediction !== lastGesture) {
                        lastGesture = data.prediction;
                        // Remove old gesture
                        resultsArea.querySelectorAll('.recognized-gesture').forEach(e => e.remove());
                        // Slide-in animated feedback
                        const gestureDiv = document.createElement('div');
                        gestureDiv.className = 'recognized-gesture';
                        gestureDiv.textContent = `Gesture recognized: ${data.prediction}`;
                        resultsArea.appendChild(gestureDiv);
                        translationWord.textContent = data.prediction;
                        translatedText.textContent = data.prediction;
                        // Voice output
                        if ('speechSynthesis' in window) {
                            const utter = new SpeechSynthesisUtterance(data.prediction);
                            utter.lang = 'en-ZA';
                            window.speechSynthesis.cancel();
                            window.speechSynthesis.speak(utter);
                        }
                    }
                })
                .catch(() => {
                    spinner.style.display = 'none';
                    waitingText.style.display = 'none';
                    resultsArea.innerHTML = '<span style="color:red">Error sending data.</span>';
                    translationWord.textContent = '---';
                    translatedText.textContent = 'Waiting for gestureâ€¦';
                });
            }
        }
        let hands;
        function setupHands() {
            hands = new window.Hands({
                locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
            });
            hands.setOptions({
                maxNumHands: 2,
                modelComplexity: 1,
                minDetectionConfidence: 0.7,
                minTrackingConfidence: 0.7
            });
            hands.onResults(onResults);
        }
        // Start Detection button logic
        const startDetectionBtn = document.getElementById('startDetectionBtn');
        let detectionActive = false;
        let animationFrameId = null;

        function startHandTracking() {
            if (!hands) return;
            detectionActive = true;
            async function processFrame() {
                if (!detectionActive) return;
                if (video.readyState === 4 && video.videoWidth > 0 && video.videoHeight > 0) {
                    resizeCanvas();
                    await hands.send({image: video});
                }
                animationFrameId = requestAnimationFrame(processFrame);
            }
            processFrame();
        }

        function stopHandTracking() {
            detectionActive = false;
            if (animationFrameId) {
                cancelAnimationFrame(animationFrameId);
                animationFrameId = null;
            }
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            webcamArea.classList.remove('glow-green', 'glow-red');
            translationWord.textContent = '---';
            translatedText.textContent = 'Waiting for gestureâ€¦';
        }

        startDetectionBtn.addEventListener('click', () => {
            if (!detectionActive) {
                startDetectionBtn.textContent = 'Stop Detection';
                startHandTracking();
            } else {
                startDetectionBtn.textContent = 'Start Detection';
                stopHandTracking();
            }
        });

        window.addEventListener('DOMContentLoaded', () => {
            setupHands();
            // Do not auto-start detection
            startDetectionBtn.textContent = 'Start Detection';
        });
    </script>
</body>
</html>
